{"cells":[{"metadata":{"cell_type":"code","id":"DA5A2D5E388D42D98220DB6D609348C8","mdEditEnable":true},"cell_type":"markdown","source":["#快手活跃用户预测——lctry队解决方案\n","##简要介绍\n","###赛题：\n","\n","“快手”新注册用户脱敏和采样后的数据30天，预测未来7天活跃的用户\n","\n","###解决方案：滑窗法\n","\n","主要使用lgb模型，xgb、catboost提升微小。另外使用了三个NN模型（keras+tensorflow）。\n","前两个NN结构相似，都是把mlp,lstm,cnn集合在一个网络中。\n","\n","lgb线下0.8905~0.891，三个NN线下都可以0.891+\n","\n","第一个NN训练方式非常对新手友好，\n","\n","第二个NN训练比较正常。\n","\n","第三个NN为GBDT特征（使用xgb提取）+deepFM，主要是对网上的开源代码做了点修改，以个人理解实现。\n","\n","（NN新手，希望有老手提提意见）\n","\n","###滑窗法对近期用户预测不准：\n","\n","另外对26-30的用户使用单天滑窗，单独提取特征，使用5个lgb模型进行单独预测\n","\n","完整的见Github: https://github.com/chantcalf/2018-Rank4-"]},{"metadata":{"id":"4A42CCD599804C278F5ADBE4977E6E3B","mdEditEnable":false},"cell_type":"markdown","source":["#1 读取数据，处理异常\n","\n","复赛异常数据好像和初赛异常数据不一样？（亏了一点点），树模型和NN融合后收益大幅度降低，因为NN对异常用户预测比较准"]},{"metadata":{"id":"DF4AD8CC6DC54AB98A429C5696A802CC"},"cell_type":"code","outputs":[],"source":["#读取数据\r\n","preg = '/mnt/datasets/fusai/user_register_log.txt'\r\n","papp = '/mnt/datasets/fusai/app_launch_log.txt'\r\n","pv = '/mnt/datasets/fusai/video_create_log.txt'\r\n","pact = '/mnt/datasets/fusai/user_activity_log.txt'\r\n","\r\n","regname = ['user_id','register_day','register_type','device_type']\r\n","actname = ['user_id','day','page','video_id','author_id','action_type']\r\n","\r\n","regd = {'user_id':'uint32','register_day':'uint8',\r\n","        'register_type':'uint8','device_type':'uint32'} \r\n","appd = {'user_id':'uint32','day':'uint8'}\r\n","actd = {'user_id':'uint32','day':'uint8',\r\n","        'page':'uint8','video_id':'uint32',\r\n","        'author_id':'uint32','action_type':'uint8'\r\n","        }\r\n","\r\n","register = pd.read_csv(preg,sep=\"\\t\",names = regname,dtype=regd)\r\n","app = pd.read_csv(papp,names=['user_id','day'],sep='\\t',dtype=appd)\r\n","video = pd.read_csv(pv,names=['user_id','day'],sep='\\t',dtype=appd)\r\n","act = pd.read_csv(pact,names=actname,sep='\\t',dtype=actd)\r\n","\r\n","#异常用户\r\n","dropa = register[(register['register_day']==24) & (register['register_type']==3) & (register['device_type']==1)][['user_id']]\r\n","dropb = register[(register['register_day']==24) & (register['register_type']==3) &(register['device_type']==83)][['user_id']]\r\n","dropc = register[(register['register_day']==24) & (register['register_type']==3) &(register['device_type']==223)][['user_id']]\r\n","dropa = dropa.append(dropb).append(dropc)\r\n","dropa['probability'] = 0  \r\n","register = register[(register['register_day']!=24) | (register['register_type']!=3) | (register['device_type']!=1)]\r\n","register = register[(register['register_day']!=24) | (register['register_type']!=3) | (register['device_type']!=83)]\r\n","register = register[(register['register_day']!=24) | (register['register_type']!=3) | (register['device_type']!=223)]\r\n","del dropb,dropc\r\n","gc.collect()"],"execution_count":null},{"metadata":{"id":"750E1170FA834A31B30C191BE721592E","mdEditEnable":false},"cell_type":"markdown","source":["#2 特征提取"]},{"metadata":{"id":"7425B08AFE2E4FA8848326BF92F0102B","mdEditEnable":false},"cell_type":"markdown","source":["##2.1 lgb特征"]},{"metadata":{"id":"7C3E878CDFE94DE1A4D667646B50A96E"},"cell_type":"code","outputs":[],"source":["def get_features_act(df,d2):\n","    df['register_time'] = d2-df.register_day+1\n","    tact = act[(act.day<=d2)]\n","    tapp = app[(app.day<=d2)]\n","    tvideo = video[(video.day<=d2)]\n","    tapp.day = d2 - tapp.day\n","    tact.day = d2 - tact.day\n","    tvideo.day = d2 - tvideo.day\n","    \n","    gp0 = tact.groupby(['user_id'])\n","    \n","    gp = gp0.size().rename('act').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left')\n","    df['act'] = df['act'].astype('float32')\n","    \n","    gp = gp0['day'].nunique().rename('act_u').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left')\n","\n","    df['act_mean'] = (df['act']/df['act_u']).astype('float32')\n","    df['act_rate'] = df['act_u']/df['register_time']\n","    \n","    gp = gp0['day'].agg({\n","        'act_day_min':min,\n","        'act_day_max':max,\n","        'act_day_mean':np.mean,\n","        'act_day_std':np.std,\n","    }).reset_index()       \n","    df = df.merge(gp,on=['user_id'],how='left')\n","    df['act_day_gap'] = (df['act_day_max'] - df['act_day_min']).astype('float16')\n","    \n","    def get_last_gap(s):\n","        s = list(s)\n","        n = len(s)\n","        if n>1:\n","            s.sort()\n","            return s[1]-s[0]\n","        return None\n","        \n","    gp = gp0['day'].unique().apply(get_last_gap).rename('act_last_gap').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left') \n","    \n","    gp = tapp[['user_id','day']].groupby(['user_id'])['day'].unique().apply(get_last_gap).rename('app_last_gap').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left')  \n","    df['app_last_gap'] = df['app_last_gap'].astype('float16')\n","    \n","    gp = tvideo[['user_id','day']].groupby(['user_id'])['day'].unique().apply(get_last_gap).rename('video_last_gap').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left') \n","    df['video_last_gap'] = df['video_last_gap'].astype('float16')\n","\n","    for i in range(7):\n","        gp = tact[tact.day<=i][['user_id']].groupby(['user_id']).size().rename('last_'+str(i)+'_days_act').reset_index()\n","        df = df.merge(gp,on=['user_id'],how='left')\n","        df['last_'+str(i)+'_days_act'] = df['last_'+str(i)+'_days_act'].fillna(0).astype('uint16')\n","\n","    for c in [0,2,3]: \n","        gp = tact[(tact.day<16) & (tact['page']==c)][['user_id']].groupby(['user_id']).size().rename('act_page_'+str(c)).reset_index()\n","        df = df.merge(gp,on=['user_id'],how='left')\n","\n","    gp = tact[(tact.day<16) ][['user_id','author_id']].groupby(['user_id'])['author_id'].nunique().rename('act_author_id_u').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left')\n","    df['act_author_id_u'] = df['act_author_id_u'].fillna(0).astype('uint32')\n","    \n","    df['act_author_id_u_mean'] = (df['act_author_id_u']/df['register_time']).astype('float32')\n","    \n","    gp = tact[(tact.day<16) ][['user_id','video_id']].groupby(['user_id'])['video_id'].nunique().rename('act_video_id_u').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left')\n","    df['act_video_id_u'] = df['act_video_id_u'].fillna(0).astype('uint32')\n","    \n","    df['act_video_id_u_mean'] = (df['act_video_id_u']/df['register_time']).astype('float32')\n","\n","    df['author_id'] = df['user_id']\n","    gp = tact[(tact.day<7) & (tact['author_id']!=tact['user_id'])].groupby(['author_id']).size().rename('author').reset_index()\n","    df = df.merge(gp,on=['author_id'],how='left')\n","\n","    gp = tact[(tact.day<7) & (tact['author_id']==tact['user_id'])][['user_id','author_id']].groupby(['user_id']).size().rename('author_self').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left')\n","\n","    gp = tact[(tact.day<7) & (tact.action_type==0)].groupby(['user_id','video_id']).size().rename('video_count').reset_index()\n","    gp1 = gp.groupby(['user_id'])['video_count'].max().rename('max_video_count').reset_index()\n","    df = df.merge(gp1,on=['user_id'],how='left')\n","    \n","    gp = tact[(tact.day<7) & (tact.action_type==0)].groupby(['user_id','author_id']).size().rename('author_count').reset_index()\n","    gp1 = gp.groupby(['user_id'])['author_count'].max().rename('max_author_count').reset_index()\n","    df = df.merge(gp1,on=['user_id'],how='left')\n","    \n","    gp = tact[(tact.day<7) & (tact.action_type==0)].groupby(['user_id','author_id'])['video_id'].nunique().rename('author_video_count').reset_index()\n","    gp1 = gp.groupby(['user_id'])['author_video_count'].max().rename('max_author_video_count').reset_index()\n","    df = df.merge(gp1,on=['user_id'],how='left')\n","    \n","    gp = tact[(tact.day<7) & (tact.page==0) & (tact.action_type==0)].groupby(['user_id'])['video_id'].nunique().rename('act00_video').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left')\n","    \n","    gp = tact[(tact.day<7) & (tact.page==0) & (tact.action_type==0)].groupby(['user_id'])['author_id'].nunique().rename('act00_author').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left')\n","\n","    gp = tact[(tact.day<7) & (tact.page==2) & (tact.action_type==0)].groupby(['user_id'])['video_id'].nunique().rename('act20_video').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left')\n","    \n","    gp = tact[(tact.day<7) & (tact.page==3) & (tact.action_type==0)].groupby(['user_id'])['author_id'].nunique().rename('act30_author').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left')\n","    \n","    gp = tact[(tact.day<7) &(tact.page==2) & (tact.action_type==2)].groupby(['user_id'])['author_id'].nunique().rename('act22').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left')\n","    \n","    gp = tact[(tact.day<7) &(tact.page==3) & (tact.action_type==2)].groupby(['user_id'])['author_id'].nunique().rename('act32').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left')\n","    del df['author_id'],df['register_day'],df['register_type'],df['device_type']\n","    print (df.shape,'act ok!')\n","    return df\n","\n","def get_features(df,d1,d2):\n","    df['register_time'] = d2-df.register_day+1\n","    tapp = app[(app.day>=d1) & (app.day<=d2)]\n","    tact = act[(act.day>=d1) & (act.day<=d2)]\n","    tvideo = video[(video.day>=d1) & (video.day<=d2)]\n","    tapp.day = d2 - tapp.day\n","    tact.day = d2 - tact.day\n","    tvideo.day = d2 - tvideo.day\n","    \n","    gp = tapp[['user_id']].groupby(['user_id']).size().rename('app').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left')\n","    df['app'] = df['app'].astype('float32')\n","    \n","    df['app_mean'] = (df['app']/df['register_time']).astype('float32')\n","    \n","    for i in range(7):\n","        gp = tapp[tapp.day<=i][['user_id']].groupby(['user_id']).size().rename('last_'+str(i)+'_days_app').reset_index()\n","        df = df.merge(gp,on=['user_id'],how='left')\n","        df['last_'+str(i)+'_days_app'] = df['last_'+str(i)+'_days_app'].astype('float32')\n","    \n","    gp = tapp[['user_id','day']].groupby(['user_id'])['day'].agg({\n","        'app_day_min':min,\n","        'app_day_max':max,\n","        'app_day_mean':np.mean,\n","        'app_day_std':np.std,\n","    }).reset_index()    \n","    df = df.merge(gp,on=['user_id'],how='left')\n","    \n","    df['app_day_gap'] = (df['app_day_max'] - df['app_day_min']).astype('float16') \n","    \n","    tapp = tapp.sort_values(by=['user_id','day'])\n","    tapp['diff'] = tapp.groupby('user_id')['day'].diff(-1).fillna(0)\n","    gp = tapp.groupby('user_id')['diff'].agg({\n","        #'app_diff_mean':np.mean,\n","        'app_diff_std':np.std,\n","        #'app_diff_max':np.max\n","    }).reset_index()\n","    df = pd.merge(df,gp,on=['user_id'],how='left',copy=False)\n","    \n","    gp = tvideo[['user_id']].groupby(['user_id']).size().rename('video').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left')\n","    df['video'] = df['video'].astype('float32')\n","    \n","    df['video_mean'] = (df['video']/df['register_time']).astype('float32')\n","\n","    gp = tvideo[['user_id','day']].groupby(['user_id'])['day'].agg({\n","        'video_day_min':min,\n","        'video_day_max':max,\n","        'video_day_mean':np.mean,\n","        'video_day_std':np.std,\n","    }).reset_index()       \n","    \n","    df = df.merge(gp,on=['user_id'],how='left')\n","    \n","    df['video_day_gap'] = (df['video_day_max'] - df['video_day_min']).astype('float16')\n","    \n","    gp = tvideo[['user_id','day']].groupby(['user_id'])['day'].nunique().rename('video_u').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left')\n","\n","    df['video_rate1'] = df['video_u']/df['app']\n","    \n","    def get_max_continue_days(s):\n","        s = list(s)\n","        s.sort()\n","        ans = 0\n","        t = 0\n","        for i in range(len(s)):\n","            if s[i]>0:\n","                t = t+1\n","            else:\n","                if t>ans:\n","                    ans = t\n","                t = 0\n","        if ans<t:\n","            ans = t\n","        return ans\n","        \n","    gp = tapp[['user_id','day']].groupby(['user_id'])['day'].unique().apply(get_max_continue_days).rename('max_continue_app').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left')\n","    \n","    gp = tact[['user_id']].groupby(['user_id']).size().rename('16act').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left')\n","\n","    for i in [0,1,2]:\n","        gp = tact[tact.page==i][['user_id']].groupby(['user_id']).size().rename('16act_page'+str(i)).reset_index()\n","        df = df.merge(gp,on=['user_id'],how='left')\n","        df['16act_page'+str(i)] = df['16act_page'+str(i)]/df['16act']\n","        \n","    for i in [0,1,2]:\n","        gp = tact[tact.action_type==i][['user_id']].groupby(['user_id']).size().rename('16act_action'+str(i)).reset_index()\n","        df = df.merge(gp,on=['user_id'],how='left')\n","        df['16act_action'+str(i)] = df['16act_action'+str(i)]/df['16act']\n","        \n","    gp = tact[tact.action_type==0][['user_id','video_id']].groupby(['user_id','video_id']).size().rename('user_video_c').reset_index()\n","    gp1 = gp[gp['user_video_c']>1][['user_id']].groupby(['user_id']).size().rename('0love_video_count').reset_index()\n","    df = df.merge(gp1,on=['user_id'],how='left')\n","    \n","    gp = tact[tact.action_type==3][['user_id','video_id']].groupby(['user_id','video_id']).size().rename('user_video_c').reset_index()\n","    gp1 = gp[gp['user_video_c']>1][['user_id']].groupby(['user_id']).size().rename('3love_video_count').reset_index()\n","    df = df.merge(gp1,on=['user_id'],how='left')\n","    \n","    gp = tact[tact.action_type==0][['user_id','video_id','author_id']].groupby(['user_id','author_id'])['video_id'].nunique().rename('uav_u').reset_index()\n","    gp1 = gp[gp['uav_u']>1][['user_id']].groupby(['user_id']).size().rename('0love_author_count').reset_index()\n","    df = df.merge(gp1,on=['user_id'],how='left')\n","    \n","    gp = tact[tact.action_type==1][['user_id','video_id','author_id']].groupby(['user_id','author_id'])['video_id'].nunique().rename('uav_u').reset_index()\n","    gp1 = gp[gp['uav_u']>1][['user_id']].groupby(['user_id']).size().rename('1love_author_count').reset_index()\n","    df = df.merge(gp1,on=['user_id'],how='left')\n","    \n","    gp = tact[tact.action_type==3][['user_id','video_id','author_id']].groupby(['user_id','author_id'])['video_id'].nunique().rename('uav_u').reset_index()\n","    gp1 = gp[gp['uav_u']>1][['user_id']].groupby(['user_id']).size().rename('3love_author_count').reset_index()\n","    df = df.merge(gp1,on=['user_id'],how='left')\n","    \n","    gp = tact[tact.day<7][tact.action_type==0][['user_id','video_id']].groupby(['user_id','video_id']).size().rename('user_video_c').reset_index()\n","    gp1 = gp[gp['user_video_c']>1][['user_id']].groupby(['user_id']).size().rename('70love_video_count').reset_index()\n","    df = df.merge(gp1,on=['user_id'],how='left')\n","    \n","    gp = tact[tact.day<7][tact.action_type==3][['user_id','video_id']].groupby(['user_id','video_id']).size().rename('user_video_c').reset_index()\n","    gp1 = gp[gp['user_video_c']>1][['user_id']].groupby(['user_id']).size().rename('73love_video_count').reset_index()\n","    df = df.merge(gp1,on=['user_id'],how='left')\n","    \n","    gp = tact[tact.day<7][tact.action_type==0][['user_id','video_id','author_id']].groupby(['user_id','author_id'])['video_id'].nunique().rename('uav_u').reset_index()\n","    gp1 = gp[gp['uav_u']>1][['user_id']].groupby(['user_id']).size().rename('70love_author_count').reset_index()\n","    df = df.merge(gp1,on=['user_id'],how='left')\n","    \n","    gp = tact[tact.day<7][tact.action_type==1][['user_id','video_id','author_id']].groupby(['user_id','author_id'])['video_id'].nunique().rename('uav_u').reset_index()\n","    gp1 = gp[gp['uav_u']>1][['user_id']].groupby(['user_id']).size().rename('71love_author_count').reset_index()\n","    df = df.merge(gp1,on=['user_id'],how='left')\n","    \n","    gp = tact[tact.day<7][tact.action_type==3][['user_id','video_id','author_id']].groupby(['user_id','author_id'])['video_id'].nunique().rename('uav_u').reset_index()\n","    gp1 = gp[gp['uav_u']>1][['user_id']].groupby(['user_id']).size().rename('73love_author_count').reset_index()\n","    df = df.merge(gp1,on=['user_id'],how='left')\n","    \n","    gp = tact[tact.action_type==0][['user_id','video_id', 'day']].groupby(['user_id','video_id'])['day'].nunique().rename('user_video_d').reset_index()\n","    gp1 = gp[gp['user_video_d']>1][['user_id']].groupby(['user_id']).size().rename('d0love_video_count').reset_index()\n","    df = df.merge(gp1,on=['user_id'],how='left')\n","    \n","    gp = tact[tact.action_type==0][['user_id','author_id', 'day']].groupby(['user_id','author_id'])['day'].nunique().rename('user_author_d').reset_index()\n","    gp1 = gp[gp['user_author_d']>1][['user_id']].groupby(['user_id']).size().rename('d0love_author_count').reset_index()\n","    df = df.merge(gp1,on=['user_id'],how='left')\n","    \n","    gp = tact[tact.day<7][tact.action_type==0][['user_id','video_id', 'day']].groupby(['user_id','video_id'])['day'].nunique().rename('user_video_d').reset_index()\n","    gp1 = gp[gp['user_video_d']>1][['user_id']].groupby(['user_id']).size().rename('7d0love_video_count').reset_index()\n","    df = df.merge(gp1,on=['user_id'],how='left')\n","    \n","    gp = tact[tact.day<7][tact.action_type==0][['user_id','author_id', 'day']].groupby(['user_id','author_id'])['day'].nunique().rename('user_author_d').reset_index()\n","    gp1 = gp[gp['user_author_d']>1][['user_id']].groupby(['user_id']).size().rename('7d0love_author_count').reset_index()\n","    df = df.merge(gp1,on=['user_id'],how='left')\n","    \n","    del df['register_time'],df['register_day'],df['register_type'],df['device_type']\n","    print (df.shape,'ok')\n","    return df\n","\n","#去除注册天再提取特征\n","def get_features1(df,d1,d2):\n","    tapp = app[(app.day<=d2)].copy()\n","    tact = act[(act.day<=d2)].copy()\n","    tvideo = video[(video.day<=d2)].copy()\n","    \n","    tapp = tapp.merge(register[['user_id','register_day']],on=['user_id'],how='left')\n","    tvideo = tvideo.merge(register[['user_id','register_day']],on=['user_id'],how='left')\n","    tact = tact.merge(register[['user_id','register_day']],on=['user_id'],how='left')\n","    \n","    tapp = tapp[tapp.day>tapp.register_day]\n","    tvideo = tvideo[tvideo.day>tvideo.register_day]\n","    tact = tact[tact.day>tact.register_day]\n","    \n","    tapp.day = d2 - tapp.day\n","    tact.day = d2 - tact.day\n","    tvideo.day = d2 - tvideo.day\n","    \n","    def getrt(x):\n","        if x<16:\n","            return x\n","        return 16\n","        \n","    df['rt'] = df['register_time'].apply(getrt)\n","    \n","    gp = tapp[tapp.day<16].groupby(['user_id']).size().rename('no_app').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left')\n","    df['no_app_r'] = df['no_app']/df['rt']\n","    \n","    temp = tact[tact.day<16].groupby(['user_id'])\n","    gp = temp.size().rename('no_act').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left')\n","    \n","    gp = temp['video_id'].nunique().rename('no_act_video').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left')\n","    gp = temp['author_id'].nunique().rename('no_act_author').reset_index()\n","    df = df.merge(gp,on=['user_id'],how='left')\n","    \n","    df['no_act_video_r'] = df['no_act_video']/df['rt']\n","    df['no_act_video_r1'] = df['no_act_video']/df['no_app']\n","    df['no_act_author_r'] = df['no_act_author']/df['rt']\n","    df['no_act_author_r1'] = df['no_act_author']/df['no_app']\n","    \n","    del temp\n","    gc.collect()\n","    \n","    del df['rt'],df['no_act_video'],df['no_act_author']\n","    print (df.shape,'ok')\n","    return df \n","\n","#注册特征\n","def get_reg_features():\n","    df = register[['user_id','register_type','device_type']]\n","    tact0 = act.merge(register[['user_id','register_day']],on=['user_id'],how='left')\n","    tvideo0 = video.merge(register[['user_id','register_day']],on=['user_id'],how='left')\n","    tapp0 = app.merge(register[['user_id','register_day']],on=['user_id'],how='left')\n","    \n","    tact = tact0[tact0.day==tact0.register_day]\n","    gp = tact.groupby(['user_id']).size().rename('regday_act').reset_index()\n","    avg_regday_act = gp['regday_act'].mean()\n","    def sp_regday_act(x):\n","        if x>=avg_regday_act:\n","            return 2\n","        return 1\n","    gp['regday_act'] = gp['regday_act'].apply(sp_regday_act)\n","    df = df.merge(gp,on=['user_id'],how='left')\n","    df['regday_act'] = df['regday_act'].fillna(0)\n","    \n","    gp = tact[tact['action_type']==2].groupby(['user_id']).size().rename('regday_action2').reset_index()\n","    gp['regday_action2'] = 1\n","    df = df.merge(gp,on=['user_id'],how='left')\n","    df['regday_action2'] = df['regday_action2'].fillna(0)\n","    \n","    for i in [0,1,3]:\n","        gp = tact[tact['action_type']==i].groupby(['user_id']).size().rename('regday_action'+str(i)).reset_index()\n","        gp['regday_action'+str(i)] = 1\n","        df = df.merge(gp,on=['user_id'],how='left')\n","        df['regday_action'+str(i)] = df['regday_action'+str(i)].fillna(0)\n","    \n","    for i in [1,2,3]:\n","        gp = tact[tact['page']==i].groupby(['user_id']).size().rename('regday_page'+str(i)).reset_index()\n","        gp['regday_page'+str(i)] = 1\n","        df = df.merge(gp,on=['user_id'],how='left')\n","        df['regday_page'+str(i)] = df['regday_page'+str(i)].fillna(0)\n","        \n","    tvideo = tvideo0[tvideo0.day==tvideo0.register_day]\n","    gp = tvideo.groupby(['user_id']).size().rename('regday_video').reset_index()\n","    avg_regday_video = gp['regday_video'].mean()\n","    def sp_regday_video(x):\n","        if x>=avg_regday_video:\n","            return 2\n","        return 1\n","    gp['regday_video'] = gp['regday_video'].apply(sp_regday_video)\n","    df = df.merge(gp,on=['user_id'],how='left')\n","    df['regday_video'] = df['regday_video'].fillna(0)\n","    \n","    print (df.shape,'ok')\n","    return df    \n","\n","#train和test的共同特征\n","def get_features_all(df,df1):\n","    print (df.shape)\n","    lendf = len(df)\n","    \n","    df= df.append(df1).reset_index(drop=True)\n","    del df1\n","    gc.collect()\n","    \n","    gp = register[['device_type']].groupby(['device_type']).size().rename('device_type_count').reset_index()\n","    gp = gp.sort_values(by='device_type_count',ascending=False)\n","    r = list(range(len(gp)))\n","    gp['r'] = r\n","    \n","    def rankfen(x):\n","        a = [5000,1000,500,0]\n","        for i in range(len(a)):\n","            if x>=a[i]:\n","                return i\n","        return -1\n","    gp['r'] = gp['r'].apply(rankfen)\n","    \n","    df = df.merge(gp,on=['device_type'],how='left')\n","    \n","    device_count = list(gp['device_type_count'])\n","    device_count.sort(reverse = True)\n","    print (len(device_count))\n","    print (device_count[10],device_count[50],device_count[100],device_count[500])\n","    \n","    df.loc[df['device_type_count']<device_count[500],'device_type_count'] = 0\n","    df.loc[(df['device_type_count']>0) & (df['device_type_count']<device_count[100]),'device_type_count'] = 1\n","    df.loc[(df['device_type_count']>0) & (df['device_type_count']<device_count[50]),'device_type_count'] = 2\n","\n","    gp = register[['device_type','register_type']].groupby(['register_type','device_type']).size().rename('device_register_count').reset_index()\n","    df = df.merge(gp,on=['register_type','device_type'],how='left')\n","    gp = register[['register_type']].groupby(['register_type']).size().rename('register_count').reset_index()\n","    df = df.merge(gp,on=['register_type'],how='left')\n","    df['device_register_count/register_count'] = df['device_register_count']/df['register_count']\n","\n","    del df['user_id'],df['register_count']\n","    df1 = df[lendf:].reset_index(drop=True)\n","    df = df[:lendf]\n","    print (df.shape)\n","    return df,df1"],"execution_count":null},{"metadata":{"id":"3BD5D55E454E4E378606425BEFBBA634","mdEditEnable":false},"cell_type":"markdown","source":["##2.2 26-30特征"]},{"metadata":{"id":"FFD017E2D2274FE388F95ABA7BA3EA6E"},"cell_type":"code","outputs":[],"source":["#预测注册k天后七天是否活跃，单天滑窗提取特征    \r\n","def get_features27(k):\r\n","    df = register[register.register_day<=23-k].reset_index(drop=True)\r\n","    test = register[register.register_day==30-k].reset_index(drop=True)\r\n","    \r\n","    app0 = pd.merge(app,register[['user_id','register_day']],on=['user_id'],how='left')\r\n","    video0 = pd.merge(video,register[['user_id','register_day']],on=['user_id'],how='left')\r\n","    act0 = pd.merge(act,register[['user_id','register_day']],on=['user_id'],how='left')\r\n","    \r\n","    app0 = app0[(app0.register_day<=23-k) | (app0.register_day==30-k)]\r\n","    video0 = video0[(video0.register_day<=23-k) | (video0.register_day==30-k)]\r\n","    act0 = act0[(act0.register_day<=23-k) | (act0.register_day==30-k)]\r\n","    \r\n","    def active30(df):\r\n","        tapp = app0[(app0['day']>app0['register_day']+k) & (app0['day']<=app0['register_day']+k+7)]\r\n","        tvideo = video0[(video0['day']>video0['register_day']+k) & (video0['day']<=video0['register_day']+k+7)]   \r\n","        tact = act0[(act0['day']>act0['register_day']+k) & (act0['day']<=act0['register_day']+k+7)]\r\n","                   \r\n","        gp1 = tapp[['user_id']].groupby(['user_id']).size().rename('app').reset_index()\r\n","        gp2 = tvideo[['user_id']].groupby(['user_id']).size().rename('video').reset_index()\r\n","        gp3 = tact[['user_id']].groupby(['user_id']).size().rename('act').reset_index()\r\n","           \r\n","        df = df.merge(gp1,on=['user_id'],how='left')\r\n","        df = df.merge(gp2,on=['user_id'],how='left')\r\n","        df = df.merge(gp3,on=['user_id'],how='left')\r\n","        \r\n","        df['app'] = df['app'].fillna(0)\r\n","        df['video'] = df['video'].fillna(0)\r\n","        df['act'] = df['act'].fillna(0)\r\n","        df['count'] = df['app']+df['video']+df['act']\r\n","        \r\n","        def isnotzero(x):\r\n","            if x>0:\r\n","                return 1\r\n","            return 0\r\n","        y = df['count'].apply(isnotzero)      \r\n","        return y\r\n","\r\n","    yy = active30(df)\r\n","    df['Y'] = yy\r\n","    test['Y'] = -1\r\n","    df = df.append(test).reset_index(drop=True)\r\n","    df['author_id'] = df['user_id']\r\n","    \r\n","    tvideo = video0[video0['day']==video0['register_day']]\r\n","    tact = act0[act0['day']==act0['register_day']]               \r\n","    '''\r\n","    gp = tvideo.groupby(['user_id']).size().rename('video').reset_index()\r\n","    df = df.merge(gp,on=['user_id'],how='left')\r\n","      \r\n","    gp = tvideo.groupby(['user_id','register_day']).size().rename('videodaycount').reset_index()\r\n","    gp1 = gp.groupby(['register_day'])['videodaycount'].mean().rename('videodaymean').reset_index()\r\n","    df = df.merge(gp1,on=['register_day'],how='left')\r\n","    df['videorate'] = (df['video']/df['videodaymean']).astype('float32')\r\n","    del df['videodaymean'],df['video']\r\n","    '''\r\n","    gp = tact[['user_id']].groupby(['user_id']).size().rename('act').reset_index()\r\n","    df = df.merge(gp,on=['user_id'],how='left')\r\n","      \r\n","    gp = tact[['user_id','register_day']].groupby(['user_id','register_day']).size().rename('actdaycount').reset_index()\r\n","    gp1 = gp.groupby(['register_day'])['actdaycount'].mean().rename('actdaymean').reset_index()\r\n","    df = df.merge(gp1,on=['register_day'],how='left')\r\n","    \r\n","    df['actrate'] = (df['act']/df['actdaymean']).astype('float32')\r\n","    del df['actdaymean']\r\n","   \r\n","    gp = tact[['user_id','author_id']].groupby(['user_id'])['author_id'].nunique().rename('author#').reset_index()\r\n","    df = df.merge(gp,on=['user_id'],how='left')\r\n","    \r\n","    gp = tact[['user_id','author_id','video_id']].groupby(['user_id','author_id'])['video_id'].nunique().rename('video_count').reset_index()\r\n","    gp1 = gp.groupby(['user_id'])['video_count'].max().rename('video_count_max').reset_index()\r\n","    df = df.merge(gp1,on=['user_id'],how='left')\r\n","    \r\n","    gp = tact[['user_id','video_id']].groupby(['user_id'])['video_id'].nunique().rename('video#').reset_index()\r\n","    df = df.merge(gp,on=['user_id'],how='left')\r\n","    \r\n","    for c in [1,2,3]:\r\n","        gp = tact[tact['page']==c][['user_id']].groupby(['user_id']).size().rename('act_page'+str(c)).reset_index()\r\n","        df = df.merge(gp,on=['user_id'],how='left')\r\n","        df['act_pagerate'+str(c)] = (df['act_page'+str(c)]/df['act']).astype('float32')\r\n","    \r\n","    for c in [1,2,3]:\r\n","        gp = tact[tact['action_type']==c][['user_id']].groupby(['user_id']).size().rename('act_action'+str(c)).reset_index()\r\n","        df = df.merge(gp,on=['user_id'],how='left')\r\n","        df['act_actionrate'+str(c)] = (df['act_action'+str(c)]/df['act']).astype('float32')\r\n","    \r\n","    gp = tact[tact['author_id']!=tact['user_id']][tact.action_type==0][['author_id']].groupby(['author_id']).size().rename('be_author0').reset_index()\r\n","    df = df.merge(gp,on=['author_id'],how='left')\r\n","    \r\n","    #gp = tact[tact['author_id']!=tact['user_id']][tact.action_type==1][['author_id']].groupby(['author_id']).size().rename('be_author1').reset_index()\r\n","    #df = df.merge(gp,on=['author_id'],how='left')\r\n","    \r\n","    gp = tact[tact['author_id']==tact['user_id']][tact.action_type==0][['author_id']].groupby(['author_id']).size().rename('self_author0').reset_index()\r\n","    df = df.merge(gp,on=['author_id'],how='left')\r\n","    \r\n","    #gp = tact[tact['author_id']==tact['user_id']][tact.action_type==3][['author_id']].groupby(['author_id']).size().rename('self_author3').reset_index()\r\n","    #df = df.merge(gp,on=['author_id'],how='left')\r\n","    \r\n","    for i in range(1,k+1):\r\n","        tapp = app0[app0['day']==app0['register_day']+i]\r\n","        tvideo = video0[video0['day']==video0['register_day']+i]\r\n","        tact = act0[act0['day']==act0['register_day']+i]               \r\n","        gp = tapp.groupby(['user_id']).size().rename(str(i)+'app').reset_index()\r\n","        df = df.merge(gp,on=['user_id'],how='left')\r\n","        \r\n","        gp = tvideo.groupby(['user_id']).size().rename(str(i)+'video').reset_index()\r\n","        df = df.merge(gp,on=['user_id'],how='left')\r\n","        \r\n","        gp = tact[['user_id']].groupby(['user_id']).size().rename(str(i)+'act').reset_index()\r\n","        df = df.merge(gp,on=['user_id'],how='left')\r\n","          \r\n","        gp = tact[['user_id','register_day']].groupby(['user_id','register_day']).size().rename('actdaycount').reset_index()\r\n","        gp1 = gp.groupby(['register_day'])['actdaycount'].mean().rename('actdaymean').reset_index()\r\n","        df = df.merge(gp1,on=['register_day'],how='left')\r\n","        \r\n","        df[str(i)+'actrate'] = (df[str(i)+'act']/df['actdaymean']).astype('float32')\r\n","        del df['actdaymean']\r\n","       \r\n","        gp = tact[['user_id','author_id']].groupby(['user_id'])['author_id'].nunique().rename(str(i)+'author#').reset_index()\r\n","        df = df.merge(gp,on=['user_id'],how='left')\r\n","        \r\n","        gp = tact[['user_id','author_id','video_id']].groupby(['user_id','author_id'])['video_id'].nunique().rename('video_count').reset_index()\r\n","        gp1 = gp.groupby(['user_id'])['video_count'].max().rename(str(i)+'video_count_max').reset_index()\r\n","        df = df.merge(gp1,on=['user_id'],how='left')\r\n","        \r\n","        gp = tact[['user_id','video_id']].groupby(['user_id'])['video_id'].nunique().rename(str(i)+'video#').reset_index()\r\n","        df = df.merge(gp,on=['user_id'],how='left')\r\n","        \r\n","        for c in [1,2,3]:\r\n","            gp = tact[tact['page']==c][['user_id']].groupby(['user_id']).size().rename(str(i)+'act_page'+str(c)).reset_index()\r\n","            df = df.merge(gp,on=['user_id'],how='left')\r\n","            #df[str(i)+'act_pagerate'+str(c)] = (df[str(i)+'act_page'+str(c)]/df[str(i)+'act']).astype('float32')\r\n","        \r\n","        for c in [1,2,3]:\r\n","            gp = tact[tact['action_type']==c][['user_id']].groupby(['user_id']).size().rename(str(i)+'act_action'+str(c)).reset_index()\r\n","            df = df.merge(gp,on=['user_id'],how='left')\r\n","            #df[str(i)+'act_actionrate'+str(c)] = (df[str(i)+'act_action'+str(c)]/df[str(i)+'act']).astype('float32')\r\n","        \r\n","        gp = tact[tact['author_id']!=tact['user_id']][tact.action_type==0][['author_id']].groupby(['author_id']).size().rename(str(i)+'be_author0').reset_index()\r\n","        df = df.merge(gp,on=['author_id'],how='left')\r\n","        \r\n","        gp = tact[tact['author_id']==tact['user_id']][tact.action_type==0][['author_id']].groupby(['author_id']).size().rename(str(i)+'self_author0').reset_index()\r\n","        df = df.merge(gp,on=['author_id'],how='left')\r\n","    \r\n","    tapp = app0[(app0['day']>app0['register_day'])&(app0['day']<app0['register_day']+4)] \r\n","    tvideo = video0[(video0['day']>video0['register_day'])&(video0['day']<video0['register_day']+4)]\r\n","    tact = act0[(act0['day']>act0['register_day'])&(act0['day']<act0['register_day']+4)] \r\n","    #tapp.day = tapp.day-tapp.register_day\r\n","    #tvideo.day = tvideo.day-tvideo.register_day\r\n","    #tact.day = tact.day-tact.register_day\r\n","    \r\n","    gp = tapp.groupby(['user_id']).size().rename('kapp').reset_index()\r\n","    df = df.merge(gp,on=['user_id'],how='left')\r\n","    \r\n","    gp = tact[['user_id']].groupby(['user_id']).size().rename('kact').reset_index()\r\n","    df = df.merge(gp,on=['user_id'],how='left')\r\n","    \r\n","    gp = tact[['user_id','author_id']].groupby(['user_id'])['author_id'].nunique().rename('kauthor#').reset_index()\r\n","    df = df.merge(gp,on=['user_id'],how='left')\r\n","    \r\n","    gp = tact[['user_id','video_id']].groupby(['user_id'])['video_id'].nunique().rename('kvideo#').reset_index()\r\n","    df = df.merge(gp,on=['user_id'],how='left')\r\n","    \r\n","    del df['author_id']\r\n","    return df    "],"execution_count":null},{"metadata":{"id":"7EF5534DC3F74CBB96DD01F3CB487FA3","mdEditEnable":false},"cell_type":"markdown","source":["##2.3 NN特征"]},{"metadata":{"id":"445FCE5833C34E369ABF6443CEBB43F6"},"cell_type":"code","outputs":[],"source":["#NN普通特征(使用时再拼上注册特征和train和test共有特征)\r\n","def get_features_nn(df,d2):\r\n","    df['register_time'] = d2-df.register_day+1\r\n","    tact = act[(act.day<=d2)]\r\n","    tapp = app[(app.day<=d2)]\r\n","    tvideo = video[(video.day<=d2)]\r\n","    tapp.day = d2 - tapp.day\r\n","    tact.day = d2 - tact.day\r\n","    tvideo.day = d2 - tvideo.day\r\n","                \r\n","    gp = tapp[['user_id']].groupby(['user_id']).size().rename('app').reset_index()\r\n","    df = df.merge(gp,on=['user_id'],how='left')\r\n","    df['app'] = df['app'].fillna(0).astype('uint8')\r\n","    df['app_mean'] = (df['app']/df['register_time']).astype('float32')\r\n","    gp = tapp[['user_id','day']].groupby(['user_id'])['day'].agg({\r\n","        'app_day_min':min,\r\n","        'app_day_max':max,\r\n","        'app_day_mean':np.mean,\r\n","        'app_day_std':np.std,\r\n","    }).reset_index()    \r\n","    df = df.merge(gp,on=['user_id'],how='left')\r\n","\r\n","    gp = tvideo[['user_id','day']].groupby(['user_id'])['day'].agg({\r\n","        'video_day_min':min,\r\n","        'video_day_max':max,\r\n","        'video_day_mean':np.mean,\r\n","        'video_day_std':np.std,\r\n","    }).reset_index()       \r\n","    \r\n","    df = df.merge(gp,on=['user_id'],how='left')\r\n","    \r\n","    gp = tvideo[['user_id']].groupby(['user_id']).size().rename('video').reset_index()\r\n","    df = df.merge(gp,on=['user_id'],how='left')\r\n","    df['video'] = df['video'].fillna(0).astype('uint16')\r\n","    \r\n","    df['video_mean'] = (df['video']/df['register_time']).astype('float32')\r\n","    \r\n","    gp = tvideo[['user_id','day']].groupby(['user_id'])['day'].nunique().rename('video_u').reset_index()\r\n","    df = df.merge(gp,on=['user_id'],how='left')\r\n","\r\n","    df['video_rate1'] = df['video_u']/df['app']\r\n","    \r\n","    gp0 = tact.groupby(['user_id'])\r\n","    gp = gp0.size().rename('act').reset_index()\r\n","    df = df.merge(gp,on=['user_id'],how='left')\r\n","    df['act'] = df['act'].fillna(0).astype('uint32')\r\n","    df['act_mean'] = (df['act']/df['register_time']).astype('float32')\r\n","    \r\n","    gp = gp0['day'].nunique().rename('act_u').reset_index()\r\n","    df = df.merge(gp,on=['user_id'],how='left')\r\n","    df['act_rate'] = df['act_u']/df['register_time']\r\n","    gp = gp0['day'].agg({\r\n","        'act_day_min':min,\r\n","        'act_day_max':max,\r\n","        'act_day_mean':np.mean,\r\n","        'act_day_std':np.std,\r\n","    }).reset_index()       \r\n","    df = df.merge(gp,on=['user_id'],how='left')\r\n","    \r\n","    def get_last_gap(s):\r\n","        s = list(s)\r\n","        n = len(s)\r\n","        if n>1:\r\n","            s.sort()\r\n","            return s[1]-s[0]\r\n","        return None\r\n","        \r\n","    gp = gp0['day'].unique().apply(get_last_gap).rename('act_last_gap').reset_index()\r\n","    df = df.merge(gp,on=['user_id'],how='left') \r\n","    \r\n","    gp = tapp[['user_id','day']].groupby(['user_id'])['day'].unique().apply(get_last_gap).rename('app_last_gap').reset_index()\r\n","    df = df.merge(gp,on=['user_id'],how='left')  \r\n","    \r\n","    gp = tvideo[['user_id','day']].groupby(['user_id'])['day'].unique().apply(get_last_gap).rename('video_last_gap').reset_index()\r\n","    df = df.merge(gp,on=['user_id'],how='left')\r\n","    \r\n","    del df['register_day']\r\n","    print('nn features ok!',df.shape)\r\n","    return df\r\n","    \r\n","#NN时序特征\r\n","def get_features_nn1(d2):\r\n","    df = register[register.register_day<=d2][['user_id']]\r\n","    tact = act[(act.day<=d2)]\r\n","    tapp = app[(app.day<=d2)]\r\n","    tvideo = video[(video.day<=d2)]\r\n","    tapp.day = d2 - tapp.day\r\n","    tact.day = d2 - tact.day\r\n","    tvideo.day = d2 - tvideo.day\r\n","    \r\n","    for i in range(14):\r\n","        t1 = tact[(tact.day==i)]\r\n","        \r\n","        gp = tapp[tapp.day==i][['user_id']].groupby(['user_id']).size().rename('app'+str(i)).reset_index()\r\n","        df = df.merge(gp,on=['user_id'],how='left')\r\n","        df['app'+str(i)] = df['app'+str(i)].fillna(0).astype('uint8')\r\n","        \r\n","        gp = t1[['user_id']].groupby(['user_id']).size().rename('act'+str(i)).reset_index()\r\n","        df = df.merge(gp,on=['user_id'],how='left')    \r\n","        df['act'+str(i)] = df['act'+str(i)].fillna(0).astype('uint16')\r\n","        \r\n","        gp = tvideo[tvideo.day==i][['user_id']].groupby(['user_id']).size().rename('video'+str(i)).reset_index()\r\n","        df = df.merge(gp,on=['user_id'],how='left')   \r\n","        df['video'+str(i)] = df['video'+str(i)].fillna(0).astype('uint16')\r\n","        \r\n","        for j in [0,1,2,3]:\r\n","            gp = t1[t1.page==j][['user_id']].groupby(['user_id']).size().rename(str(i)+'act_page'+str(j)).reset_index()\r\n","            df = df.merge(gp,on=['user_id'],how='left')\r\n","            df[str(i)+'act_pager'+str(j)] = (df[str(i)+'act_page'+str(j)]/(df['act'+str(i)]+0.00001)).astype('float32')\r\n","            \r\n","        for j in [0,1,2,3]:\r\n","            gp = t1[t1.action_type==j][['user_id']].groupby(['user_id']).size().rename(str(i)+'act_action'+str(j)).reset_index()\r\n","            df = df.merge(gp,on=['user_id'],how='left')\r\n","            df[str(i)+'act_actionr'+str(j)] = (df[str(i)+'act_action'+str(j)]/(df['act'+str(i)]+0.00001)).astype('float32')    \r\n","        \r\n","        for j in [0,1,2,3]:\r\n","            t2 = t1[t1.page==j]\r\n","            for k in [0,1,2,3]:\r\n","                if j==1 and k==2:\r\n","                    continue\r\n","                gp =  t2[t2.action_type==k][['user_id']].groupby(['user_id']).size().rename('act'+str(i)+str(j)+str(k)).reset_index()\r\n","                df = df.merge(gp,on=['user_id'],how='left')\r\n","                df['act'+str(i)+str(j)+str(k)] = df['act'+str(i)+str(j)+str(k)].fillna(0).astype('uint16')\r\n","                \r\n","    print('nn features ok!',df.shape)\r\n","    return df   \r\n","    "],"execution_count":null},{"metadata":{"id":"0B8E498ED594493F99D7B8EDDBE0403B","mdEditEnable":false},"cell_type":"markdown","source":["##2.4 GBDT特征\n","使用xgb生成100（树的数量）维类别特征，因为树深为4，最多16个叶子，但取值不是0-15，使用前需LabelEncoder"]},{"metadata":{"id":"318D631DCC4D4DBF80FB1BA4A358D63E"},"cell_type":"code","outputs":[],"source":["def gbdt_lr_predict_data(train_df,train_y,test_df):\r\n","    model = xgb.XGBClassifier(\r\n","        nthread=8,# cpu 线程数 默认最大\r\n","        n_estimators=100,\r\n","        max_depth=4,\r\n","        seed=5,\r\n","        learning_rate=0.07,\r\n","        subsample=0.9,\r\n","        min_child_weight=1,\r\n","        colsample_bytree=.9,\r\n","        gamma = 0.1,\r\n","        reg_alpha=0.7,\r\n","        reg_lambda=0.6,\r\n","        silent=False,\r\n","        eval_metric='auc'\r\n","        )\r\n","    model.fit(train_df,train_y)\r\n","    grd_res = model.predict(test_df) \r\n","    #grd_enc = OneHotEncoder()\r\n","    train_data = model.apply(train_df)\r\n","    print (train_data.shape)\r\n","    test_data = model.apply(test_df)\r\n","    print (test_data.shape)\r\n","    return train_data,test_data"],"execution_count":null},{"metadata":{"id":"74435E1A9BC24E7E9F0179864432EAE7","mdEditEnable":false},"cell_type":"markdown","source":["#3 树模型\n","普通的lgb，xgb，catboost，具体代码见github\n",""]},{"metadata":{"id":"1DC199F9580142DDB12E32AEF7092FF4","mdEditEnable":false},"cell_type":"markdown","source":["#4 NN"]},{"metadata":{"id":"0DF494874D054024827FD839F16417A2","mdEditEnable":false},"cell_type":"markdown","source":["#4.1 NN0\n","双输入模型，普通特征+时序特征\n","\n","时序特征按时间顺序提取的，手动拉成二维\n","\n","MLP使用 init='he_normal'，和PRelu效果比默认和Relu好些\n","\n","BN和dropout是标配，BN正常在激活函数前面，但也可以放在激活函数之后，有时有提升"]},{"metadata":{"id":"57C31F68EDAE4C2F803CF59B2B1F2B08"},"cell_type":"code","outputs":[],"source":["def simple_keras0(train_df,dfa,train_y,test_df,dfb,val,y2):\r\n","    train_df =train_df.fillna(-1)#普通特征\r\n","    test_df = test_df.fillna(-1)\r\n","    dfa =dfa.fillna(0)#时序特征\r\n","    dfb = dfb.fillna(0)\r\n","\r\n","    sc = StandardScaler()\r\n","    sc.fit(train_df)\r\n","    X = sc.transform(train_df)\r\n","    test_df = sc.transform(test_df)\r\n","    y = train_y.values\r\n","    \r\n","    sc.fit(dfa)\r\n","    X1 = sc.transform(dfa.values)\r\n","    test_df1 = sc.transform(dfb.values)\r\n","    \r\n","    def getlstmdata(s,s0):\r\n","        n = s.shape[1]\r\n","        m = n//14\r\n","        data = []\r\n","        for i in range(13,-1,-1):\r\n","            idx = list(range(m*(i),m*(i+1)))\r\n","            #temp = np.append(s[:,idx],s0,axis=1)\r\n","            data.append(s[:,idx])\r\n","   \r\n","        data = np.array(data)\r\n","        data = np.swapaxes(data,0,1)\r\n","        #data = np.swapaxes(data,1,2)\r\n","        print (data.shape)\r\n","        return data\r\n","    \r\n","    X1 = getlstmdata(X1,X)\r\n","    test_df1 = getlstmdata(test_df1,test_df)\r\n","        \r\n","    def get_model1():\r\n","        xin = Input(shape=(X.shape[1],))\r\n","        xin1 = Input(shape=(X1.shape[1],X1.shape[2]))\r\n","        \r\n","        x1 = Dense(32,init='he_normal')(xin)\r\n","\r\n","        x1 = PReLU()(x1)\r\n","        x1b = x1\r\n","        x1 = BatchNormalization()(x1)\r\n","        x1 = Dropout(0.2)(x1)\r\n","        x1 = Dense(16,init='he_normal')(x1)\r\n","        x1 = PReLU()(x1)\r\n","        x1 = BatchNormalization()(x1)\r\n","        x1 = Dropout(0.25)(x1)\r\n","\r\n","        x1 = Dense(1)(x1)\r\n"," \r\n","        #x2 = Dense(64)(xin1)\r\n","        #x2 = PReLU()(x2)\r\n","        x2 = LSTM(36,activation='relu',dropout=0.2,return_sequences=True)(xin1)#多对多\r\n","        #x2 = LSTM(16,activation='relu',dropout=0.2,return_sequences=True)(x2)\r\n","        x2 = LSTM(18,activation='relu',dropout=0.2,return_sequences=False)(x2)#多对一\r\n","        x2b = x2\r\n","        #x2 = PReLU()(x2)\r\n","        x2 = BatchNormalization()(x2)\r\n","        x2 = Dropout(0.5)(x2)\r\n","        x2 = Dense(1)(x2)\r\n","        \r\n","        #可以堆多个一维卷积，但是效果不好，这里没加\r\n","        #可以用for循环，计算多种滤波尺寸的CNN，最后再拼接\r\n","        x3 = Conv1D(20,3)(xin1)#可以加same\r\n","        x3 = BatchNormalization()(x3)\r\n","        x3 = Activation('relu')(x3)\r\n","        x3 = MaxPooling1D(2)(x3)\r\n","        x3 = LSTM(10,activation='relu',dropout=0.2,return_sequences=False)(x3)\r\n","        #x3 = Conv1D(64,3)(x3)\r\n","        #x3 = BatchNormalization()(x3)\r\n","        #x3 = Activation('relu')(x3)\r\n","        #x3 = Lambda(lambda x: K.max(x,axis = 1))(x3)\r\n","        x3 = Dropout(0.5)(x3)\r\n","        x3 = Dense(1)(x3)\r\n","    \r\n","        xb = Concatenate(axis=1)([x1b,x2b])\r\n","        xb = Dense(16,init='he_normal')(xb)\r\n","        xb = PReLU()(xb)\r\n","        xb = BatchNormalization()(xb)\r\n","        xb = Dropout(0.25)(xb)\r\n","        \r\n","        xb = Dense(1)(xb)\r\n","        \r\n","        xx = Concatenate(axis=1)([x1,x2,xb,x3])\r\n","        y = Dense(1, activation='sigmoid')(xx)\r\n","        \r\n","        model = Model(inputs=[xin,xin1],outputs=y)\r\n","        return model\r\n","\r\n","    from sklearn.model_selection import KFold\r\n","    N = 5\r\n","    skf = KFold(n_splits=N,shuffle=False,random_state=None)\r\n","    \r\n","    xx_cv = []\r\n","    xx_pre = []\r\n","    ii = 0\r\n","    \r\n","    for train_in,test_in in skf.split(X,y):\r\n","        X_train,X_test,y_train,y_test = X[train_in],X[test_in],y[train_in],y[test_in]\r\n","        X_train1 = X1[train_in]\r\n","        X_test1 = X1[test_in]\r\n","        \r\n","        model = get_model1()\r\n","        model.compile(loss='binary_crossentropy',\r\n","                      #optimizer=SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False),\r\n","                      optimizer = 'nadam',\r\n","                      #metrics=['accuracy']\r\n","                      )\r\n","        if ii==0:            \r\n","            print (model.summary())\r\n","        \r\n","        epochs = 3\r\n","        filepath = 'best_param'+str(ii)+'.h5'\r\n","        bests = 0\r\n","        beste = 0\r\n","        allres = []\r\n","        allval = []\r\n","\r\n","        #for循环观察每轮结果，判断是否保存参数，是否更改学习率等等，手动挡，很灵活\r\n","        for i in range(20):\r\n","            print (i)\r\n","            model.fit([X_train,X_train1],y_train,epochs=1,batch_size=512,\r\n","                    validation_data=[[X_test,X_test1],y_test],\r\n","                    shuffle=True,verbose=2,)\r\n","            res = model.predict([X_test,X_test1],batch_size=1024)\r\n","            s = metrics.roc_auc_score(y_test, res)\r\n","            \r\n","            res1 = model.predict([test_df,test_df1],batch_size=1024)\r\n","            allres.append(res1)\r\n","            allval.append(s)\r\n","            print (s)\r\n","            if val:\r\n","                print(metrics.roc_auc_score(y2, res1))\r\n","            if s>bests:\r\n","                bests = s\r\n","                model.save(filepath)\r\n","                beste = i\r\n","            if i-beste>5:\r\n","                break       \r\n","        model.load_weights(filepath)    \r\n","        model.compile(loss='binary_crossentropy',\r\n","                      optimizer=SGD(lr=0.0004, momentum=0.9, nesterov=False),\r\n","                      #optimizer = 'nadam',\r\n","                      #metrics=['accuracy']\r\n","                      )\r\n","        for i in range(5):\r\n","            print (i)\r\n","            model.fit([X_train,X_train1],y_train,epochs=1,batch_size=512,\r\n","                    validation_data=[[X_test,X_test1],y_test],\r\n","                    shuffle=True,verbose=2,)\r\n","            res = model.predict([X_test,X_test1],batch_size=1000)\r\n","            s = metrics.roc_auc_score(y_test, res)\r\n","            \r\n","            res1 = model.predict([test_df,test_df1],batch_size=1000)\r\n","            allres.append(res1)\r\n","            allval.append(s)\r\n","            print (s)\r\n","            if val:\r\n","                print(metrics.roc_auc_score(y2, res1))\r\n","            if s>bests:\r\n","                bests = s\r\n","                model.save(filepath)\r\n","                beste = i\r\n","            if i-beste>10:\r\n","                break       \r\n","        #model.save(filepath)    \r\n","        model.load_weights(filepath)\r\n","        res1 = model.predict([test_df,test_df1],batch_size=1000)\r\n","        if val:\r\n","            s1 = metrics.roc_auc_score(y2, res1)\r\n","            print (s1)              \r\n","        xx_pre.append(res1)  \r\n","        xx_cv.append(bests)\r\n","        ii = ii + 1\r\n","        #if val:\r\n","        #break\r\n","    \r\n","    if val:\r\n","        test_y2 = xx_pre[0]\r\n","    else:\r\n","        test_y2 = 0\r\n","        for i in xx_pre:\r\n","            test_y2 = test_y2+i\r\n","        test_y2 = test_y2/N\r\n","    \r\n","    if val:\r\n","        print (metrics.roc_auc_score(y2, test_y2))\r\n","\r\n","    return test_y2"],"execution_count":null},{"metadata":{"id":"BF77747B46E14CEAAC9FDD5A3EADBA09","mdEditEnable":false},"cell_type":"markdown","source":["#4.2 NN1"]},{"metadata":{"id":"2BACD60BB18E4633B5B402D8EF89D728"},"cell_type":"code","outputs":[],"source":["#划分比较粗糙,这个auc会比实际值小\r\n","def auc(y_true, y_pred):  \r\n","    ptas = tf.stack([binary_PTA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)  \r\n","    pfas = tf.stack([binary_PFA(y_true,y_pred,k) for k in np.linspace(0, 1, 1000)],axis=0)  \r\n","    pfas = tf.concat([tf.ones((1,)) ,pfas],axis=0)  \r\n","    binSizes = -(pfas[1:]-pfas[:-1])  \r\n","    s = ptas*binSizes  \r\n","    return K.sum(s, axis=0)  \r\n","\r\n","# PFA, prob false alert for binary classifier  \r\n","def binary_PFA(y_true, y_pred, threshold=K.variable(value=0.5)):  \r\n","    y_pred = K.cast(y_pred >= threshold, 'float32')  \r\n","    # N = total number of negative labels  \r\n","    N = K.sum(1 - y_true)  \r\n","    # FP = total number of false alerts, alerts from the negative class labels  \r\n","    FP = K.sum(y_pred - y_pred * y_true)  \r\n","    return FP/N  \r\n","\r\n","# P_TA prob true alerts for binary classifier  \r\n","def binary_PTA(y_true, y_pred, threshold=K.variable(value=0.5)):  \r\n","    y_pred = K.cast(y_pred >= threshold, 'float32')  \r\n","    # P = total number of positive labels  \r\n","    P = K.sum(y_true)  \r\n","    # TP = total number of correct alerts, alerts from the positive class labels  \r\n","    TP = K.sum(y_pred * y_true)  \r\n","    return TP/P \r\n","\r\n","#train\r\n","def simple_keras(train_df,dfa,train_y,test_df,dfb,val,y2,numk):\r\n","    train_df =train_df.fillna(-1)\r\n","    test_df = test_df.fillna(-1)\r\n","    dfa =dfa.fillna(0)\r\n","    dfb = dfb.fillna(0)\r\n","\r\n","    sc = StandardScaler()\r\n","    sc.fit(train_df)\r\n","    X = sc.transform(train_df)\r\n","    test_df = sc.transform(test_df)\r\n","    y = train_y.values\r\n","    \r\n","    sc.fit(dfa)\r\n","    X1 = sc.transform(dfa.values)\r\n","    test_df1 = sc.transform(dfb.values)\r\n","    \r\n","    def getlstmdata(s,s0):\r\n","        n = s.shape[1]\r\n","        m = n//14\r\n","        data = []\r\n","        for i in range(13,-1,-1):\r\n","            # = list(range(m*(i+1)-1,m*(i)-1,-1))\r\n","            idx = list(range(m*(i),m*(i+1)))\r\n","            #temp = np.append(s[:,idx],s0,axis=1)\r\n","            data.append(s[:,idx])\r\n","   \r\n","        data = np.array(data)\r\n","        data = np.swapaxes(data,0,1)\r\n","        #data = np.swapaxes(data,1,2)\r\n","        print (data.shape)\r\n","        return data\r\n","    \r\n","    X1 = getlstmdata(X1,X)\r\n","    test_df1 = getlstmdata(test_df1,test_df)\r\n","    \r\n","    \r\n","    def get_model_nn1():\r\n","        xin = Input(shape=(X.shape[1],))\r\n","        xin1 = Input(shape=(X1.shape[1],X1.shape[2]))\r\n","        \r\n","        x1 = Dense(32,init='he_normal')(xin)\r\n","        x1 = PReLU()(x1)\r\n","        x1 = BatchNormalization()(x1)\r\n","        x1b = x1\r\n","\r\n","        x2 = LSTM(36,activation='relu',dropout=0.2,return_sequences=False)(xin1)\r\n","        x2 = BatchNormalization()(x2)\r\n","        x2b = x2\r\n","\r\n","        x3 = Conv1D(20,3)(xin1)\r\n","        x3 = BatchNormalization()(x3)\r\n","        x3 = Activation('relu')(x3)\r\n","        x3 = MaxPooling1D(2)(x3)\r\n","        x3 = LSTM(10,activation='relu',dropout=0.2,return_sequences=False)(x3)\r\n","        x3 = BatchNormalization()(x3)\r\n","        x3b = x3\r\n","        \r\n","        xb = Concatenate(axis=1)([x1b,x2b,x3b])\r\n","        xb = Dense(64,init='he_normal')(xb)\r\n","        xb = PReLU()(xb)\r\n","        xb = BatchNormalization()(xb)\r\n","        xb = Dropout(0.25)(xb)\r\n","        xb = Dense(16,init='he_normal')(xb)\r\n","        xb = Dropout(0.5)(xb)\r\n","        y = Dense(1, activation='sigmoid')(xb)\r\n","        \r\n","        model = Model(inputs=[xin,xin1],outputs=y)\r\n","        return model\r\n","\r\n","    #十折平均效果较差，可以使用随机数，每次随机生成随机种子，从中随机取一种划分方式，跑多个NN再平均\r\n","    from sklearn.model_selection import KFold\r\n","    N = 10\r\n","    skf = KFold(n_splits=N,shuffle=True,random_state=888)\r\n","    xx_cv = []\r\n","    xx_pre = []\r\n","    ii = 0\r\n","    for train_in,test_in in skf.split(X,y):\r\n","        if ii<9:\r\n","            ii = ii+1\r\n","            continue\r\n","        X_train,X_test,y_train,y_test = X[train_in],X[test_in],y[train_in],y[test_in]\r\n","        X_train1 = X1[train_in]\r\n","        X_test1 = X1[test_in]\r\n","        \r\n","        model = get_model_nn1()\r\n","        model.compile(loss='binary_crossentropy',\r\n","                      #optimizer=SGD(lr=0.1, momentum=0.9, decay=0.0, nesterov=False),\r\n","                      optimizer = 'nadam',\r\n","                      #optimizer = 'nadam',\r\n","                      metrics=[auc]\r\n","                      )\r\n","        #if ii==0:            \r\n","            #print (model.summary())\r\n","        filepath = 'model_nn1_param'+str(numk)+'.h5'\r\n","        bests = 0\r\n","        beste = 0\r\n","        allres = []\r\n","        allval = []\r\n","        checkpoint = ModelCheckpoint(filepath, monitor='val_auc', verbose=1, save_best_only=True, mode='max')\r\n","        \r\n","        model.fit([X_train,X_train1],y_train,epochs=20,batch_size=1024,\r\n","                    validation_data=[[X_test,X_test1],y_test],\r\n","                    #validation_data=[[test_df,test_df1],y2],\r\n","                    shuffle=True,verbose=2,\r\n","                    callbacks = [EarlyStopping(monitor='val_auc',patience=5,mode='max'),checkpoint],\r\n","                    )\r\n","        \r\n","        model.load_weights(filepath)\r\n","        res = model.predict([X_test,X_test1],batch_size=4096)\r\n","        s = metrics.roc_auc_score(y_test, res)\r\n","        bests = s\r\n","        print (s)\r\n","        if val:\r\n","            res1 = model.predict([test_df,test_df1],batch_size=4096)\r\n","            print(metrics.roc_auc_score(y2, res1))\r\n","            \r\n","        model.compile(loss='binary_crossentropy',\r\n","                      optimizer=SGD(lr=0.0001, momentum=0.9,decay=0, nesterov=False),\r\n","                      metrics=[auc]\r\n","                      )\r\n","        for i in range(10):\r\n","            print (i)\r\n","            model.fit([X_train,X_train1],y_train,epochs=1,batch_size=1024,\r\n","                    validation_data=[[X_test,X_test1],y_test],\r\n","                    shuffle=True,verbose=2,)\r\n","            res = model.predict([X_test,X_test1],batch_size=1024)\r\n","            s = metrics.roc_auc_score(y_test, res)\r\n","            \r\n","            res1 = model.predict([test_df,test_df1],batch_size=4096)\r\n","            allres.append(res1)\r\n","            allval.append(s)\r\n","            print (s)\r\n","            if val:\r\n","                print(metrics.roc_auc_score(y2, res1))\r\n","            if s>bests:\r\n","                bests = s\r\n","                model.save(filepath)\r\n","                beste = i\r\n","            if i-beste>=2:\r\n","                break   \r\n","               \r\n","        #model.save(filepath)    \r\n","        model.load_weights(filepath)\r\n","        res1 = model.predict([test_df,test_df1],batch_size=4096)\r\n","        if val:\r\n","            s1 = metrics.roc_auc_score(y2, res1)\r\n","            print (s1)              \r\n","        xx_pre.append(res1)  \r\n","        xx_cv.append(bests)\r\n","        ii = ii + 1\r\n","        #if val:\r\n","        break\r\n","    \r\n","    test_y2 = xx_pre[0]\r\n","    return test_y2"],"execution_count":null},{"metadata":{"id":"430A9FB4E82E4772852F93CD027F48B6","mdEditEnable":false},"cell_type":"markdown","source":["#4.3 GBDT+deepFM\n","输入为100维类别特征\n","deepFM就是一阶+二阶+MLP\n","可以用onehot+Dense(1)代替embedding，但是参数更多，更慢"]},{"metadata":{"id":"1B95D5F7CCC74D338709F51CE399582A"},"cell_type":"code","outputs":[],"source":["class MySumLayer(Layer):\r\n","    def __init__(self, axis, **kwargs):\r\n","        self.supports_masking = True\r\n","        self.axis = axis\r\n","        super(MySumLayer, self).__init__(**kwargs)\r\n","\r\n","    def compute_mask(self, input, input_mask=None):\r\n","        # do not pass the mask to the next layers\r\n","        return None\r\n","\r\n","    def call(self, x, mask=None):\r\n","\r\n","        if mask is not None:\r\n","            # mask (batch, time)\r\n","            mask = K.cast(mask, K.floatx())\r\n","            if K.ndim(x)!=K.ndim(mask):\r\n","                mask = K.repeat(mask, x.shape[-1])\r\n","                mask = tf.transpose(mask, [0,2,1])\r\n","            x = x * mask\r\n","            if K.ndim(x)==2:\r\n","                x = K.expand_dims(x)\r\n","            return K.sum(x, axis=self.axis)\r\n","        else:\r\n","            if K.ndim(x)==2:\r\n","                x = K.expand_dims(x)\r\n","            return K.sum(x, axis=self.axis)\r\n","\r\n","    def compute_output_shape(self, input_shape):\r\n","        output_shape = []\r\n","        for i in range(len(input_shape)):\r\n","            if i!=self.axis:\r\n","                output_shape.append(input_shape[i])\r\n","        if len(output_shape)==1:\r\n","            output_shape.append(1)\r\n","        return tuple(output_shape)\r\n","\r\n","class MyFlatten(Layer):\r\n","    def __init__(self, **kwargs):\r\n","        self.supports_masking = True\r\n","        super(MyFlatten, self).__init__(**kwargs)\r\n","\r\n","    def compute_mask(self, inputs, mask=None):\r\n","        if mask==None:\r\n","            return mask\r\n","        return K.batch_flatten(mask)\r\n","\r\n","    def call(self, inputs, mask=None):\r\n","        return K.batch_flatten(inputs)\r\n","\r\n","    def compute_output_shape(self, input_shape):\r\n","        return (input_shape[0], np.prod(input_shape[1:]))\r\n","\r\n","from sklearn import preprocessing\r\n","def simple_nn(train_df,train_y,test_df,val,y2):\r\n","\r\n","    nums = list(train_df.nunique())\r\n","    le = preprocessing.LabelEncoder()\r\n","    for c in train_df.columns:\r\n","        le.fit(train_df[c])\r\n","        train_df[c] = le.transform(train_df[c])\r\n","        test_df[c] = le.transform(test_df[c])\r\n","    \r\n","    X = train_df.values\r\n","    y = train_y.values\r\n","\r\n","    def slices(x):\r\n","        t = []\r\n","        for i in range(x.shape[1]):\r\n","            ti = K.expand_dims(x[:,i],1)\r\n","            t.append(ti)\r\n","        return t\r\n","            \r\n","    def embed(x):\r\n","        t = []\r\n","        for i in x:\r\n","            tx = Dense(1,use_bias=False)(i)\r\n","            t.append(tx)\r\n","        t = Concatenate(axis=1)(t)\r\n","        return t\r\n","        \r\n","    def catembed0(x):\r\n","        t = []\r\n","        for i in range(len(x)):\r\n","            tx = Embedding(nums[i],1,input_length=1)(x[i])\r\n","            t.append(tx)\r\n","        t = Add()(t)    \r\n","        return Flatten()(t)\r\n","        \r\n","    def catembed(x):#每个特征最多16个取值，压缩为二维向量\r\n","        t = []\r\n","        for i in range(len(x)):\r\n","            tx = Embedding(nums[i],2,input_length=1)(x[i])\r\n","            t.append(tx)\r\n","        t = Concatenate(axis=1)(t)\r\n","        return t\r\n","    \r\n","    def get_deepfm():\r\n","        xin = Input(shape=(X.shape[1],))\r\n","        \r\n","        #把特征分成一列列各做embeding\r\n","        x0 = Lambda(slices)(xin)\r\n","        #一阶\r\n","        fd = catembed0(x0)\r\n","        \r\n","        #二阶\r\n","        x0e = catembed(x0)\r\n","        x0s = MySumLayer(axis=1)(x0e)\r\n","        x02 = Multiply()([x0e, x0e])\r\n","        x02s = MySumLayer(axis=1)(x02)\r\n","        x0s2 = Multiply()([x0s, x0s])\r\n","        sd = Subtract()([x0s2,x02s])\r\n","        sd = Lambda(lambda x:x*0.5)(sd)\r\n","        sd = MySumLayer(axis=1)(sd)\r\n","        \r\n","        #MLP\r\n","        x1 = MyFlatten()(x0e)   \r\n","        x1 = Dense(64,init='he_normal')(x1)\r\n","        x1 = PReLU()(x1)\r\n","        x1 = BatchNormalization()(x1)\r\n","        x1 = Dropout(0.5)(x1)\r\n","        x1 = Dense(64,init='he_normal')(x1)\r\n","        x1 = PReLU()(x1)\r\n","        x1 = BatchNormalization()(x1)\r\n","        x1 = Dropout(0.5)(x1)\r\n","        x1 = Dense(64,init='he_normal')(x1)\r\n","        x1 = PReLU()(x1)\r\n","        x1 = BatchNormalization()(x1)\r\n","        x1 = Dropout(0.5)(x1)\r\n","        x1 = Dense(1, activation='relu')(x1)\r\n","    \r\n","        x1 = Concatenate(axis=1)([fd, sd,x1])\r\n","        y = Dense(1, activation='sigmoid')(x1)\r\n","        \r\n","        model = Model(inputs=xin,outputs=y)\r\n","        return model\r\n","\r\n","    from sklearn.model_selection import KFold\r\n","    N = 10\r\n","    skf = KFold(n_splits=N,shuffle=False,random_state=None)\r\n","    \r\n","    xx_cv = []\r\n","    xx_pre = []\r\n","    ii = 0\r\n","    \r\n","    for train_in,test_in in skf.split(X,y):\r\n","        X_train,X_test,y_train,y_test = X[train_in],X[test_in],y[train_in],y[test_in]\r\n","        \r\n","        model = get_deepfm()\r\n","        model.compile(loss='binary_crossentropy',\r\n","                      optimizer = 'nadam',\r\n","                      metrics=[auc]\r\n","                      )\r\n","        if ii==0:            \r\n","            print (model.summary())\r\n","        \r\n","        filepath = fp0+'xgb_deepfm_param'+str(ii)+'.h5'\r\n","        bests = 0\r\n","        beste = 0\r\n","        allres = []\r\n","        allval = []\r\n","        \r\n","        checkpoint = ModelCheckpoint(filepath, monitor='val_auc', verbose=1, save_best_only=True, mode='max')\r\n","        \r\n","        model.fit(X_train,y_train,epochs=2,batch_size=512,\r\n","                    validation_data=[X_test,y_test],\r\n","                    #validation_data=[test_df,y2],\r\n","                    shuffle=True,verbose=2,callbacks = [EarlyStopping(monitor='val_auc',patience=5,mode='max'),checkpoint])\r\n","        \r\n","        model.load_weights(filepath)\r\n","        res1 = model.predict(test_df,batch_size=1000)\r\n","        if val:\r\n","            s1 = metrics.roc_auc_score(y2, res1)\r\n","            print (s1)              \r\n","        xx_pre.append(res1)  \r\n","        xx_cv.append(bests)\r\n","        ii = ii + 1\r\n","    \r\n","    if val:\r\n","        test_y2 = xx_pre[0]\r\n","    else:\r\n","        test_y2 = 0\r\n","        for i in xx_pre:\r\n","            test_y2 = test_y2+i\r\n","        test_y2 = test_y2/N\r\n","    return test_y2"],"execution_count":null},{"metadata":{"id":"F9ACDBE8676E42F6826BB2083CC4CCEF"},"cell_type":"code","outputs":[],"source":[""],"execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python","nbconvert_exporter":"python","file_extension":".py","version":"3.5.2","pygments_lexer":"ipython3"}},"nbformat":4,"nbformat_minor":0}